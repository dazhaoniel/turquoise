{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5737d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/swin-tiny-patch4-window7-224\" # pre-trained model from which to fine-tune\n",
    "batch_size = 32 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bc3e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60cff179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe420d73396946e9bb3b4af943dffb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c74341b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !sudo apt -qq install git-lfs\n",
    "# !git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9662de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the images and labels of gemstones from training data\n",
    "def extract_labels(name):\n",
    "    parts = name.split('/')\n",
    "    train = parts[1]\n",
    "    if 'Fake' in parts[2]:\n",
    "        label = 'fake'\n",
    "    else:\n",
    "        label = 'real'\n",
    "    return train, label\n",
    "    \n",
    "\n",
    "def read_images():\n",
    "    Images = []\n",
    "    for root, dirs, files in os.walk('datasets'):\n",
    "        f = os.path.basename(root)\n",
    "        for file in files:\n",
    "            try:\n",
    "                image = cv2.imread(root+'/'+file)\n",
    "                image = np.array(image)\n",
    "                train, label = extract_labels(root)\n",
    "                Images.append([train, image, label])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return Images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdc0c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = read_images()\n",
    "df = pd.DataFrame(images, columns = ['train', 'image', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88faf971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>[[[152, 156, 157], [152, 156, 157], [152, 156,...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>[[[152, 156, 157], [152, 156, 157], [152, 156,...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>[[[152, 156, 157], [152, 156, 157], [152, 156,...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>[[[152, 156, 157], [152, 156, 157], [152, 156,...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>[[[152, 156, 157], [152, 156, 157], [152, 156,...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train                                              image label\n",
       "0  test  [[[152, 156, 157], [152, 156, 157], [152, 156,...  fake\n",
       "1  test  [[[152, 156, 157], [152, 156, 157], [152, 156,...  fake\n",
       "2  test  [[[152, 156, 157], [152, 156, 157], [152, 156,...  fake\n",
       "3  test  [[[152, 156, 157], [152, 156, 157], [152, 156,...  fake\n",
       "4  test  [[[152, 156, 157], [152, 156, 157], [152, 156,...  fake"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62c3cad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91c12943b2f448da0b765975024d13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19958c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded106a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9ea7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
